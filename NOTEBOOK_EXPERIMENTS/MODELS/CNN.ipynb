{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"final.csv\", low_memory=False)\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 31,
          "data": {
            "text/plain": "   Sl no                 Name   Profile ID  \\\n0      1         scotty2jatty    398274122   \n1      2     angelasanders975  65963290409   \n2      3        myleslewis_24  11687291106   \n3      4  miladheydarpour1369   7529960718   \n4      5  matthew_williams224  25240207448   \n\n                               Comment            Comment_Tokens  \\\n0                  you better do dirty              better,dirty   \n1              thats soooooooooo right   thats,soooooooooo,right   \n2                bro i want that shirt            bro,want,shirt   \n3                                 good                      good   \n4  bro he only needs to beat jakes ass  bro,needs,beat,jakes,ass   \n\n   comment_length  word_count  sentiment_score sentiment_class  \n0              19           4                0         neutral  \n1              23           3                0         neutral  \n2              21           5                1        positive  \n3               4           1                3        positive  \n4              35           8               -7        negative  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Name</th>\n      <th>Profile ID</th>\n      <th>Comment</th>\n      <th>Comment_Tokens</th>\n      <th>comment_length</th>\n      <th>word_count</th>\n      <th>sentiment_score</th>\n      <th>sentiment_class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>scotty2jatty</td>\n      <td>398274122</td>\n      <td>you better do dirty</td>\n      <td>better,dirty</td>\n      <td>19</td>\n      <td>4</td>\n      <td>0</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>angelasanders975</td>\n      <td>65963290409</td>\n      <td>thats soooooooooo right</td>\n      <td>thats,soooooooooo,right</td>\n      <td>23</td>\n      <td>3</td>\n      <td>0</td>\n      <td>neutral</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>myleslewis_24</td>\n      <td>11687291106</td>\n      <td>bro i want that shirt</td>\n      <td>bro,want,shirt</td>\n      <td>21</td>\n      <td>5</td>\n      <td>1</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>miladheydarpour1369</td>\n      <td>7529960718</td>\n      <td>good</td>\n      <td>good</td>\n      <td>4</td>\n      <td>1</td>\n      <td>3</td>\n      <td>positive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>matthew_williams224</td>\n      <td>25240207448</td>\n      <td>bro he only needs to beat jakes ass</td>\n      <td>bro,needs,beat,jakes,ass</td>\n      <td>35</td>\n      <td>8</td>\n      <td>-7</td>\n      <td>negative</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1738700755513
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a mapping dictionary for sentiment classes\n",
        "sentiment_mapping = {\n",
        "    'neutral': 1,\n",
        "    'positive': 1,\n",
        "    'negative': 0\n",
        "}\n",
        "\n",
        "# Apply the mapping to the 'sentiment_class' column to create a numerical 'sentiment_label'\n",
        "df['sentiment_label'] = df['sentiment_class'].map(sentiment_mapping)\n",
        "\n",
        "# Display the first few rows to confirm\n",
        "print(df[['sentiment_label', 'sentiment_class']].head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "   sentiment_label sentiment_class\n0                1         neutral\n1                1         neutral\n2                1        positive\n3                1        positive\n4                0        negative\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1738700756408
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "   Sl no                 Name   Profile ID  \\\n0      1         scotty2jatty    398274122   \n1      2     angelasanders975  65963290409   \n2      3        myleslewis_24  11687291106   \n3      4  miladheydarpour1369   7529960718   \n4      5  matthew_williams224  25240207448   \n\n                               Comment            Comment_Tokens  \\\n0                  you better do dirty              better,dirty   \n1              thats soooooooooo right   thats,soooooooooo,right   \n2                bro i want that shirt            bro,want,shirt   \n3                                 good                      good   \n4  bro he only needs to beat jakes ass  bro,needs,beat,jakes,ass   \n\n   comment_length  word_count  sentiment_score sentiment_class  \\\n0              19           4                0         neutral   \n1              23           3                0         neutral   \n2              21           5                1        positive   \n3               4           1                3        positive   \n4              35           8               -7        negative   \n\n   sentiment_label  \n0                1  \n1                1  \n2                1  \n3                1  \n4                0  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Sl no</th>\n      <th>Name</th>\n      <th>Profile ID</th>\n      <th>Comment</th>\n      <th>Comment_Tokens</th>\n      <th>comment_length</th>\n      <th>word_count</th>\n      <th>sentiment_score</th>\n      <th>sentiment_class</th>\n      <th>sentiment_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>scotty2jatty</td>\n      <td>398274122</td>\n      <td>you better do dirty</td>\n      <td>better,dirty</td>\n      <td>19</td>\n      <td>4</td>\n      <td>0</td>\n      <td>neutral</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>angelasanders975</td>\n      <td>65963290409</td>\n      <td>thats soooooooooo right</td>\n      <td>thats,soooooooooo,right</td>\n      <td>23</td>\n      <td>3</td>\n      <td>0</td>\n      <td>neutral</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>myleslewis_24</td>\n      <td>11687291106</td>\n      <td>bro i want that shirt</td>\n      <td>bro,want,shirt</td>\n      <td>21</td>\n      <td>5</td>\n      <td>1</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>miladheydarpour1369</td>\n      <td>7529960718</td>\n      <td>good</td>\n      <td>good</td>\n      <td>4</td>\n      <td>1</td>\n      <td>3</td>\n      <td>positive</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>matthew_williams224</td>\n      <td>25240207448</td>\n      <td>bro he only needs to beat jakes ass</td>\n      <td>bro,needs,beat,jakes,ass</td>\n      <td>35</td>\n      <td>8</td>\n      <td>-7</td>\n      <td>negative</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1738700757236
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# General libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random as rn\n",
        "\n",
        "# Neural network libraries\n",
        "import tensorflow as tf  # TensorFlow backend\n",
        "from tensorflow.keras.layers import Input  # For input layer\n",
        "from tensorflow.keras.layers import Dropout  # For random dropout\n",
        "from tensorflow.keras.layers import Embedding  # For embedding\n",
        "from tensorflow.keras.layers import Conv1D, GlobalMaxPooling1D  # For convolution layer\n",
        "from tensorflow.keras.layers import concatenate  # For concatenation\n",
        "from tensorflow.keras.layers import Activation  # For activation layer\n",
        "from tensorflow.keras.layers import Dense  # For fully connected layer\n",
        "from tensorflow.keras.models import Model  # Model groups layers into an object with training and inference features.\n",
        "\n",
        "# Libraries for data formatting\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Word embedding loading library\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Suppress TensorFlow info logs\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# To ensure TensorFlow uses only the CPU\n",
        "tf.config.set_visible_devices([], 'GPU')\n",
        "\n",
        "print(\"Using TensorFlow version:\", tf.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Using TensorFlow version: 2.18.0\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1738700757930
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEED = 123\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)"
      ],
      "outputs": [],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1738700758571
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting into train and test sets (80/20 split)\n",
        "df_train = df.sample(frac=0.8, random_state=123)  # 80% of the data\n",
        "df_test = df.drop(df_train.index)  # Remaining 20% of the data\n",
        "\n",
        "# Selecting only the 'Comment' and 'sentiment_class' columns\n",
        "df_train = df_train[[\"Comment\", \"sentiment_label\"]]\n",
        "df_test = df_test[[\"Comment\", \"sentiment_label\"]]\n",
        "\n",
        "df_train, df_test"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 36,
          "data": {
            "text/plain": "(                                                Comment  sentiment_label\n 4933  you said good bye dumb bitch snd i said okwhat...                0\n 184                                        amen to that                1\n 2399  tevin is my favorite sophomore i think hes so ...                1\n 1520          trump means king of truth and pragmatisme                1\n 3519                             who that pussy wet for                0\n ...                                                 ...              ...\n 1924                                               good                1\n 806   he stands for diversity equity inclusion and t...                0\n 1808                                               good                1\n 1076                                       debate trump                1\n 1261  im disgusted with the people who still vote fo...                0\n \n [4071 rows x 2 columns],\n                                                 Comment  sentiment_label\n 2                                 bro i want that shirt                1\n 3                                                  good                1\n 8          yeah boy tysons gonna get shit done the goat                0\n 14                                i cant wait to see it                1\n 16    you do have a money plan get punched in the mo...                1\n ...                                                 ...              ...\n 5066                               bitch ass nigga rico                0\n 5078       kendrick is not better than papoose or drake                1\n 5080                 mixtape sell sole got some bangers                1\n 5081  i was lucky to be sensible and ive never had a...                1\n 5086               she might be the one dont be a bitch                0\n \n [1018 rows x 2 columns])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1738700759150
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.groupby('sentiment_label').count()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 37,
          "data": {
            "text/plain": "                 Comment\nsentiment_label         \n0                   1909\n1                   2162",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n    </tr>\n    <tr>\n      <th>sentiment_label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1909</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2162</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1738700759762
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_train)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 38,
          "data": {
            "text/plain": "4071"
          },
          "metadata": {}
        }
      ],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1738700760210
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# separate independent variabe (text) and dependednt variable (label)\n",
        "X_train, y_train = df_train['Comment'].values, df_train['sentiment_label'].values"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1738700760738
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train[:5]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 40,
          "data": {
            "text/plain": "array(['you said good bye dumb bitch snd i said okwhat is it with you white devilsi have nothing for you vampire lover of blood',\n       'amen to that',\n       'tevin is my favorite sophomore i think hes so funny hes really not lmaoo hes a little hoe',\n       'trump means king of truth and pragmatisme',\n       'who that pussy wet for'], dtype=object)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1738700761297
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check what we got in dependent variabl\n",
        "y_train[:5]"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 41,
          "data": {
            "text/plain": "array([0, 1, 1, 1, 0])"
          },
          "metadata": {}
        }
      ],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1738700761977
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 42,
          "data": {
            "text/plain": "                                              Comment  sentiment_label\n2                               bro i want that shirt                1\n3                                                good                1\n8        yeah boy tysons gonna get shit done the goat                0\n14                              i cant wait to see it                1\n16  you do have a money plan get punched in the mo...                1",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Comment</th>\n      <th>sentiment_label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2</th>\n      <td>bro i want that shirt</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>good</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>yeah boy tysons gonna get shit done the goat</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>i cant wait to see it</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>you do have a money plan get punched in the mo...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1738700762776
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# seperate variabes of test data\n",
        "X_test, y_test = df_test['Comment'].values, df_test['sentiment_label'].values"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1738700763371
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = 100000\n",
        "tokenizer = Tokenizer(num_words=num_words)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "xtrain = tokenizer.texts_to_sequences(X_train) # coverts text to numbers\n",
        "print(xtrain[:2])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[3, 95, 11, 703, 195, 7, 3023, 5, 95, 3024, 8, 16, 23, 3, 140, 3025, 28, 212, 13, 3, 3026, 1949, 10, 985], [1144, 4, 9]]\n"
        }
      ],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1738700764183
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen = max(map(lambda x: len(x), xtrain))\n",
        "xtrain = pad_sequences(xtrain, maxlen=maxlen) # padding each row to make them same length\n",
        "print(xtrain[:2])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "[[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    3   95   11  703  195    7 3023    5   95\n  3024    8   16   23    3  140 3025   28  212   13    3 3026 1949   10\n   985]\n [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0    0    0\n     0    0    0    0    0    0    0    0    0    0    0    0 1144    4\n     9]]\n"
        }
      ],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1738700764884
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(maxlen)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "393\n"
        }
      ],
      "execution_count": 68,
      "metadata": {
        "gather": {
          "logged": 1738701329676
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the text of test data\n",
        "xtest = tokenizer.texts_to_sequences(X_test) # coverts text to numbers\n",
        "xtest = pad_sequences(xtest, maxlen=maxlen) "
      ],
      "outputs": [],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1738700765552
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "\n",
        "glove_file = 'glove.6B.50d.txt'  \n",
        "word2vec_file = 'glove.6B.50d_w2v.txt'  \n",
        "\n",
        "\n",
        "import os\n",
        "if not os.path.exists(glove_file):\n",
        "    print(f\"Error: GloVe file not found at {glove_file}\")\n",
        "else:\n",
        "    # Convert GloVe format to Word2Vec format\n",
        "    try:\n",
        "        glove2word2vec(glove_file, word2vec_file)\n",
        "        print(f\"Successfully converted GloVe file to Word2Vec format: {word2vec_file}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error during conversion: {e}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_3406/2388528531.py:14: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n  glove2word2vec(glove_file, word2vec_file)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Successfully converted GloVe file to Word2Vec format: glove.6B.50d_w2v.txt\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1738700784550
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Load the Word2Vec model\n",
        "word2vec_model = KeyedVectors.load_word2vec_format('glove.6B.50d_w2v.txt', binary=False)\n",
        "\n",
        "# Check the model to verify it's loaded properly\n",
        "print(\"Word2Vec model loaded successfully!\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Word2Vec model loaded successfully!\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1738700793294
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "embedding_dim = 50  # Dimension of the embeddings (corresponding to 'glove.6B.50d.txt')\n",
        "\n",
        "# Assuming you have a tokenizer and it is already fitted\n",
        "num_words = min(10000, len(tokenizer.word_index))  # Limit the size of the vocabulary\n",
        "\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i >= num_words:\n",
        "        continue\n",
        "    embedding_vector = word2vec_model.get_vector(word) if word in word2vec_model else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Embedding matrix shape: (7458, 50)\n"
        }
      ],
      "execution_count": 52,
      "metadata": {
        "gather": {
          "logged": 1738700809961
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 53,
          "data": {
            "text/plain": "array([[   0,    0,    0, ..., 1949,   10,  985],\n       [   0,    0,    0, ..., 1144,    4,    9],\n       [   0,    0,    0, ...,    2,  169,   41],\n       ...,\n       [   0,    0,    0, ...,    0,    0,   11],\n       [   0,    0,    0, ...,    0,  575,   44],\n       [   0,    0,    0, ...,  148,   13,   94]], dtype=int32)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 53,
      "metadata": {
        "gather": {
          "logged": 1738700814252
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xtrain_clipped = np.clip(xtrain, 0, num_words - 1)"
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {
        "gather": {
          "logged": 1738700815457
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Dense, Dropout\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.layers import Input, concatenate, Activation\n",
        "from keras.models import Model\n",
        "\n",
        "def create_cnn_model():\n",
        "    comment_input = Input(shape=(maxlen,), dtype='int32')\n",
        "    \n",
        "    print('loading word vectors')\n",
        "    # Use the pre-trained embedding matrix for the Embedding layer\n",
        "    comment_encoder = Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True)(comment_input)\n",
        "    comment_encoder = Dropout(0.5)(comment_encoder)\n",
        "    \n",
        "    # Bigram branch\n",
        "    bigram_branch = Conv1D(filters=128, kernel_size=3, padding='valid', activation='relu', strides=1)(comment_encoder)\n",
        "    bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
        "    bigram_branch = Dropout(0.5)(bigram_branch)\n",
        "    \n",
        "    # Trigram branch\n",
        "    trigram_branch = Conv1D(filters=256, kernel_size=4, padding='valid', activation='relu', strides=1)(comment_encoder)\n",
        "    trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
        "    trigram_branch = Dropout(0.2)(trigram_branch)\n",
        "    \n",
        "    # Fourgram branch\n",
        "    fourgram_branch = Conv1D(filters=512, kernel_size=5, padding='valid', activation='relu', strides=1)(comment_encoder)\n",
        "    fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
        "    fourgram_branch = Dropout(0.2)(fourgram_branch)\n",
        "    \n",
        "    # Merging the branches\n",
        "    merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
        "\n",
        "    # Fully connected layer\n",
        "    merged = Dense(256, activation='relu')(merged)\n",
        "    merged = Dropout(0.5)(merged)\n",
        "    \n",
        "    # Output layer\n",
        "    merged = Dense(1)(merged)\n",
        "    output = Activation('sigmoid')(merged)\n",
        "    \n",
        "    # Create and compile the model\n",
        "    model = Model(inputs=[comment_input], outputs=[output])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "# Assuming `xtrain`, `y_train`, `embedding_matrix`, and other required variables are already defined\n",
        "cnn_model = create_cnn_model()\n",
        "cnn_model.fit(xtrain_clipped, y_train, epochs=3, batch_size=32, verbose=1)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py38/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n/anaconda/envs/azureml_py38/lib/python3.10/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\nExpected: ['keras_tensor_17']\nReceived: inputs=Tensor(shape=(None, 393))\n  warnings.warn(msg)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.6063 - loss: 0.7495\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 2/3\n\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.7541 - loss: 0.4898\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nEpoch 3/3\n\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 28ms/step - accuracy: 0.8051 - loss: 0.4226\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 55,
          "data": {
            "text/plain": "<keras.src.callbacks.history.History at 0x7efb727ad810>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 55,
      "metadata": {
        "gather": {
          "logged": 1738700830046
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
        "from sklearn.metrics import classification_report\n",
        "import numpy as np\n",
        "import datetime\n",
        "\n",
        "def evaluate_binary_classification(gnd_truths, predictions, model_name='', path=''):\n",
        "    # Convert to numpy arrays if not already\n",
        "    gnd_truths = np.array(gnd_truths)\n",
        "    predictions = np.array(predictions)\n",
        "    \n",
        "    # Calculate basic metrics\n",
        "    tp = np.count_nonzero(predictions * gnd_truths)\n",
        "    tn = np.count_nonzero((predictions - 1) * (gnd_truths - 1))\n",
        "    fp = np.count_nonzero(predictions * (gnd_truths - 1))\n",
        "    fn = np.count_nonzero((predictions - 1) * gnd_truths)\n",
        "    \n",
        "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
        "    precision = tp / (tp + fp)\n",
        "    recall = tp / (tp + fn)\n",
        "    f1measure = (2 * precision * recall) / (precision + recall)\n",
        "    \n",
        "    cks = cohen_kappa_score(predictions, gnd_truths)\n",
        "    false_positive_rate, true_positive_rate, thresholds = roc_curve(gnd_truths, predictions)\n",
        "    auc_ = auc(false_positive_rate, true_positive_rate)\n",
        "    \n",
        "    # Basic Report\n",
        "    basic_report = {\n",
        "        'Total Samples': len(gnd_truths),\n",
        "        'Positive Samples': sum(gnd_truths),\n",
        "        'Negative Samples': len(gnd_truths) - sum(gnd_truths),\n",
        "        'True Positive': tp,\n",
        "        'True Negative': tn,\n",
        "        'False Positive': fp,\n",
        "        'False Negative': fn,\n",
        "        'Accuracy': accuracy,\n",
        "        'Precision': precision,\n",
        "        'Recall': recall,\n",
        "        'F1 Measure': f1measure,\n",
        "        'Cohen Kappa Score': cks,\n",
        "        'Area Under Curve': auc_\n",
        "    }\n",
        "    \n",
        "    # Full Report\n",
        "    full_report = get_str(basic_report) + '\\n'\n",
        "    full_report += classification_report(gnd_truths, predictions)\n",
        "    \n",
        "    # Save Reports if path is provided\n",
        "    if path:\n",
        "        save_report(basic_report, 'basic_report.txt', path)\n",
        "        save_report(full_report, 'full_report.txt', path)\n",
        "    \n",
        "    return basic_report, full_report\n",
        "\n",
        "\n",
        "def get_str(report):\n",
        "    \"\"\"Helper function to convert a report dictionary to string format.\"\"\"\n",
        "    str_ = ''\n",
        "    for key in report.keys():\n",
        "        str_ += f'{key}\\t{report[key]}\\n'\n",
        "    return str_\n",
        "\n",
        "\n",
        "def save_report(report, filename, path):\n",
        "    \"\"\"Helper function to save the report to a file.\"\"\"\n",
        "    with open(path + filename, 'a+') as FO:\n",
        "        FO.write(report)\n",
        "\n",
        "# Example usage\n",
        "# You will provide gnd_truths and predictions here\n",
        "# gnd_truths = [0, 1, 1, 0, 1]\n",
        "# predictions = [0, 1, 0, 0, 1]\n",
        "\n",
        "# basic_report, full_report = evaluate_binary_classification(gnd_truths, predictions)\n",
        "\n",
        "# Printing the reports\n",
        "# print(\"Basic Report:\")\n",
        "# print(basic_report)\n",
        "\n",
        "# print(\"\\nFull Report:\")\n",
        "# print(full_report)\n"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "gather": {
          "logged": 1738700843400
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming cnn_model, xtest, and y_test are already defined\n",
        "p = cnn_model.predict(xtest, verbose=1)\n",
        "predicted = [int(round(x[0])) for x in p]  # Convert the predictions to 0 or 1\n",
        "actual = y_test  # Actual ground truth labels\n",
        "\n",
        "# Call the evaluate_binary_classification function\n",
        "basic_report, full_report = evaluate_binary_classification(gnd_truths=actual, predictions=predicted)\n",
        "\n",
        "# Print the full report\n",
        "print(full_report)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m22/32\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m29/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\nTotal Samples\t1018\nPositive Samples\t522\nNegative Samples\t496\nTrue Positive\t437\nTrue Negative\t413\nFalse Positive\t83\nFalse Negative\t85\nAccuracy\t0.8349705304518664\nPrecision\t0.8403846153846154\nRecall\t0.8371647509578544\nF1 Measure\t0.8387715930902112\nCohen Kappa Score\t0.6697587839560049\nArea Under Curve\t0.8349130206402174\n\n              precision    recall  f1-score   support\n\n           0       0.83      0.83      0.83       496\n           1       0.84      0.84      0.84       522\n\n    accuracy                           0.83      1018\n   macro avg       0.83      0.83      0.83      1018\nweighted avg       0.83      0.83      0.83      1018\n\n"
        }
      ],
      "execution_count": 57,
      "metadata": {
        "gather": {
          "logged": 1738700852428
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.summary()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1mModel: \"functional_1\"\u001b[0m\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m393\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ -                 │\n│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_1         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m393\u001b[0m, \u001b[38;5;34m50\u001b[0m)   │    \u001b[38;5;34m372,900\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n│ (\u001b[38;5;33mEmbedding\u001b[0m)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m393\u001b[0m, \u001b[38;5;34m50\u001b[0m)   │          \u001b[38;5;34m0\u001b[0m │ embedding_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m391\u001b[0m, \u001b[38;5;34m128\u001b[0m)  │     \u001b[38;5;34m19,328\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_4 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m390\u001b[0m, \u001b[38;5;34m256\u001b[0m)  │     \u001b[38;5;34m51,456\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_5 (\u001b[38;5;33mConv1D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m389\u001b[0m, \u001b[38;5;34m512\u001b[0m)  │    \u001b[38;5;34m128,512\u001b[0m │ dropout_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ conv1d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n│ (\u001b[38;5;33mGlobalMaxPooling1…\u001b[0m │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ global_max_pooli… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m896\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dropout_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ dropout_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n│                     │                   │            │ dropout_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │    \u001b[38;5;34m229,632\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_9 (\u001b[38;5;33mDropout\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)       │          \u001b[38;5;34m0\u001b[0m │ dense_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │        \u001b[38;5;34m257\u001b[0m │ dropout_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]     │\n│ (\u001b[38;5;33mActivation\u001b[0m)        │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">393</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ embedding_1         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">393</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)   │    <span style=\"color: #00af00; text-decoration-color: #00af00\">372,900</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">393</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)   │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ embedding_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">391</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">19,328</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">51,456</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ conv1d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">389</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)  │    <span style=\"color: #00af00; text-decoration-color: #00af00\">128,512</span> │ dropout_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ global_max_pooling… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv1d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalMaxPooling1…</span> │                   │            │                   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_max_pooli… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dropout_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ dropout_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n│                     │                   │            │ dropout_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │    <span style=\"color: #00af00; text-decoration-color: #00af00\">229,632</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dropout_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)       │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">257</span> │ dropout_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]     │\n│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │                   │            │                   │\n└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,406,257\u001b[0m (9.18 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,406,257</span> (9.18 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m802,085\u001b[0m (3.06 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">802,085</span> (3.06 MB)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m1,604,172\u001b[0m (6.12 MB)\n",
            "text/html": "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,604,172</span> (6.12 MB)\n</pre>\n"
          },
          "metadata": {}
        }
      ],
      "execution_count": 64,
      "metadata": {
        "gather": {
          "logged": 1738701102577
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_sentiment(model, tokenizer, text_input, maxlen):\n",
        "    \"\"\"\n",
        "    Predict sentiment for a single text input or a batch of text inputs.\n",
        "    \n",
        "    Args:\n",
        "    - model: The trained model (CNN in your case)\n",
        "    - tokenizer: The tokenizer used to preprocess text\n",
        "    - text_input: A single text string or a list of text strings\n",
        "    - maxlen: Maximum length of input sequences (padding length)\n",
        "    \n",
        "    Returns:\n",
        "    - predictions: A list of predicted sentiment labels (0 for negative, 1 for positive)\n",
        "    \"\"\"\n",
        "    print(f\"Received input text: {text_input}\")  # Debugging line\n",
        "    \n",
        "    if isinstance(text_input, str):\n",
        "        # If a single text string, make it a list\n",
        "        text_input = [text_input]\n",
        "\n",
        "    # Convert the input texts to sequences\n",
        "    text_sequences = tokenizer.texts_to_sequences(text_input)\n",
        "    print(f\"Text sequences after tokenization: {text_sequences}\")  # Debugging line\n",
        "    \n",
        "    # Pad the sequences to ensure uniform length\n",
        "    text_padded = pad_sequences(text_sequences, maxlen=maxlen)\n",
        "    print(f\"Padded sequences shape: {text_padded.shape}\")  # Debugging line\n",
        "    \n",
        "    # Make predictions\n",
        "    try:\n",
        "        predictions = model.predict(text_padded)\n",
        "        print(f\"Model predictions: {predictions}\")  # Debugging line\n",
        "    except Exception as e:\n",
        "        print(f\"Error during prediction: {e}\")\n",
        "        return None\n",
        "\n",
        "    # Convert probabilities to binary sentiment labels (0 or 1)\n",
        "    sentiment_labels = (predictions > 0.5).astype(int)\n",
        "    print(f\"Predicted sentiment labels: {sentiment_labels}\")  # Debugging line\n",
        "    \n",
        "    return sentiment_labels.flatten()\n",
        "\n",
        "# User input loop for continuous prediction\n",
        "def user_input_loop(model, tokenizer, maxlen):\n",
        "    print(\"Starting prediction loop...\")\n",
        "    while True:\n",
        "        # Ask the user to input a comment\n",
        "        input_text = input(\"Enter a comment to predict sentiment (or 'exit' to quit): \")\n",
        "        \n",
        "        if input_text.lower() == 'exit':\n",
        "            print(\"Exiting prediction loop.\")\n",
        "            break\n",
        "        \n",
        "        # Make prediction for the entered text\n",
        "        sentiment = predict_sentiment(model, tokenizer, input_text, maxlen)\n",
        "        \n",
        "        # Check if prediction was successful\n",
        "        if sentiment is not None:\n",
        "            sentiment_label = \"Positive\" if sentiment[0] == 1 else \"Negative\"\n",
        "            # Output the result\n",
        "            print(f\"The sentiment of the comment is: {sentiment_label}\")\n",
        "        else:\n",
        "            print(\"Prediction failed.\")\n",
        "        \n",
        "        print(\"-\" * 50)\n",
        "\n",
        "# Debugging Step: Check if the model is loaded properly\n",
        "print(\"Loading model...\")\n",
        "# Assuming cnn_model and tokenizer are already loaded or created in the script\n",
        "print(\"Model loaded:\", cnn_model)\n",
        "print(\"Tokenizer loaded:\", tokenizer)\n",
        "\n",
        "# Example usage\n",
        "# Assuming cnn_model, tokenizer, and maxlen are already defined\n",
        "user_input_loop(cnn_model, tokenizer, 393)  # Update maxlen to 393\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Loading model...\nModel loaded: <Functional name=functional_1, built=True>\nTokenizer loaded: <keras.src.legacy.preprocessing.text.Tokenizer object at 0x7efbcf0a2a70>\nStarting prediction loop...\nReceived input text: nigga\nText sequences after tokenization: [[72]]\nPadded sequences shape: (1, 393)\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\nModel predictions: [[0.4261403]]\nPredicted sentiment labels: [[0]]\nThe sentiment of the comment is: Negative\n--------------------------------------------------\nExiting prediction loop.\n"
        }
      ],
      "execution_count": 69,
      "metadata": {
        "gather": {
          "logged": 1738701509499
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save('CNN_NEW.h5')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
        }
      ],
      "execution_count": 70,
      "metadata": {
        "gather": {
          "logged": 1738701509736
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# Save the tokenizer to a pickle file\n",
        "with open('CNN_tokenizer.pickle', 'wb') as handle:\n",
        "    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "outputs": [],
      "execution_count": 71,
      "metadata": {
        "gather": {
          "logged": 1738701538342
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1738687773660
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python38-azureml",
      "language": "python",
      "display_name": "Python 3.8 - AzureML"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python38-azureml"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}